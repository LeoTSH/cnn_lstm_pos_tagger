{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, keras, string, itertools, random, datetime, numpy as np, matplotlib.pyplot as plt, tensorflow as tf\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import glorot_uniform, random_uniform\n",
    "from keras.layers import Activation\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Embedding, Conv1D, Flatten, Dense, Dropout, LSTM, Bidirectional, TimeDistributed, \\\n",
    "Dropout, Input, concatenate, Reshape\n",
    "from keras import regularizers\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom functions\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    '''\n",
    "    Description: Prints and plots the confusion matrix.\tNormalization can be applied by setting `normalize=True`\n",
    "\n",
    "    Args:\n",
    "        - cm: Confusion Matrix\n",
    "        - classes: Names of classes\n",
    "        - normalize: Whether to or to not normal values in Confusion Matrix\n",
    "        - cmap: Plot color\n",
    "    '''\n",
    "\n",
    "    # Check if normalize is true or false\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    # Format axis and plot Confusion Matrix\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "def get_labels(seq):\n",
    "    '''\n",
    "    Description: Creates a sequence of labels based on the input sequence\n",
    "\n",
    "    Args:\n",
    "        - seq: Input sequence\n",
    "    \n",
    "    Returns:\n",
    "        Sequence labels\n",
    "    '''\n",
    "    \n",
    "    labels_seq = []\n",
    "    seq = seq.split()\n",
    "    for i in range(len(seq)):\n",
    "        if '...' in seq[i]:\n",
    "            labels_seq.append('<3-dots>')\n",
    "        elif ',' in seq[i]:\n",
    "            labels_seq.append('<comma>')\n",
    "        elif '.' in seq[i]:\n",
    "            labels_seq.append('<period>')\n",
    "        elif '?' in seq[i]:\n",
    "            labels_seq.append('<question>')\n",
    "        elif '!' in seq[i]:\n",
    "            labels_seq.append('<exclaim>')\n",
    "        else:\n",
    "            labels_seq.append('<na>')\n",
    "    return labels_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model parameters\n",
    "name_1 = 'ted-glove-lstm'\n",
    "name_2 = 'ted-glove-cnn-lstm'\n",
    "model_name = name_1\n",
    "embed_dim = 300\n",
    "max_seq_len = 128\n",
    "drop_prob = 0.35\n",
    "filter_sizes = [64,64,64]\n",
    "kernels = [3,5,7]\n",
    "kernel_weight = glorot_uniform(seed=50)\n",
    "bias = glorot_uniform(seed=50)\n",
    "kernel_reg = regularizers.l2(l=0.0001)\n",
    "lstm_hidden = 2048\n",
    "lstm_hidden_2 = 1024\n",
    "adam_lr = 0.001\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "valid_split = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set misc parameters\n",
    "current = datetime.datetime.now()\n",
    "date = current.strftime('%b-%d')\n",
    "tensor_b = TensorBoard(log_dir='./tf_logs/model_{}_hidden_{}_dropout_{}_embed_dim_{}_lr_{}'.format(model_name, \n",
    "                        lstm_hidden, drop_prob,\n",
    "                        embed_dim, adam_lr), \n",
    "                        batch_size=batch_size, \n",
    "                        write_graph=True, histogram_freq=0)\n",
    "early_s = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "class_names = ['Pad', 'NA', 'Comma', 'Period', 'Question', 'Exclaim', '3-Dots']\n",
    "\n",
    "# Look-up table to remove punctuations from data\n",
    "table = str.maketrans('', '', punctuation)\n",
    "\n",
    "# Remove characters\n",
    "replace = ['♫', '♪', '–', '…', '(applause)', '(laughter)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and process input/label data\n",
    "# Read and load dataset\n",
    "data = open('./data/processed/ted_data', 'r', encoding='utf-8').read()\n",
    "\n",
    "# Convert all characters to lowercase\n",
    "data = data.lower()\n",
    "\n",
    "# Remove unnecessary characters\n",
    "for i in range(len(replace)):\n",
    "    data = data.replace(replace[i], ' ')\n",
    "\n",
    "# Group sentences into list\n",
    "data_split = data.split('\\n')\n",
    "print('Pre number of sentences:', len(data_split))\n",
    "print('\\n')\n",
    "# Get longest sentence in dataset and its index\n",
    "print(max(enumerate(data_split), key=lambda x: len(x[1])))\n",
    "print('Length of longest sentence:', len(max(data_split, key=len)))\n",
    "\n",
    "# Clean/format longest sentence\n",
    "data_split[185073] = data_split[185073].replace(',', ', ')\n",
    "data_split[185073] = data_split[185073].replace('.', '.\\n')\n",
    "long_sent = data_split[185073].split('\\n')\n",
    "\n",
    "# Check number of sentences after chunking longest sentence\n",
    "print('Chunked longest sentence:', len(long_sent))\n",
    "\n",
    "# Remove data at index 185703\n",
    "del data_split[185073]\n",
    "\n",
    "# Add chunked sentences back to dataset\n",
    "for x in long_sent:\n",
    "    data_split.append(x)\n",
    "\n",
    "# Remove empty rows\n",
    "data_split = data_split[:238003]\n",
    "\n",
    "# Check ending of dataset\n",
    "print('Last Sentence', data_split[-1])\n",
    "print('\\n')\n",
    "\n",
    "# Check length of dataset after addition\n",
    "print('Post number of sentences:', len(data_split))\n",
    "print('\\n')\n",
    "\n",
    "# Get sequence labels\n",
    "process_labels = [get_labels(seq) for seq in data_split]\n",
    "process_labels = [' '.join(seq) for seq in process_labels]\n",
    "\n",
    "# Remove punctuations\n",
    "sequences = [seq.translate(table) for seq in data_split]\n",
    "\n",
    "# Combined sentences back into a single piece for Counter\n",
    "combined_sequences = ' '.join(sequences)\n",
    "\n",
    "# Check if there are characters to remove\n",
    "print(Counter(combined_sequences))\n",
    "print('\\n')\n",
    "    \n",
    "# Get all words in the dataset\n",
    "words = combined_sequences.split()\n",
    "\n",
    "# Records inputs and labels for reference\n",
    "with open('./data/processed/processed_input', 'w', encoding='utf-8') as f:\n",
    "    for x in sequences:\n",
    "        f.write(x+'\\n')\n",
    "\n",
    "with open('./data/processed/processed_labels', 'w', encoding='utf-8') as f:\n",
    "    for x in process_labels:\n",
    "        f.write(x+'\\n')\n",
    "\n",
    "# Check number of sequences and labels\n",
    "print('Number of sequences: \\t{}'.format(len(sequences)))\n",
    "print('Number of labels: \\t{}'.format(len(process_labels)))\n",
    "\n",
    "# Load processed labels\n",
    "y_labels = open('./data/processed/processed_labels', 'r', encoding='utf-8').read()\n",
    "y_labels = y_labels.split('\\n')\n",
    "y_labels = y_labels[:-1]\n",
    "all_labels = ' '.join(y_labels)\n",
    "labels_tag = all_labels.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build words vocab\n",
    "all_data = ' '.join(sequences)\n",
    "words = all_data.split()\n",
    "words_in_vocab = Counter(words)\n",
    "vocab = sorted(words_in_vocab, key=words_in_vocab.get, reverse=True)\n",
    "\n",
    "# Skip most common word\n",
    "vocab_to_int = {word: index for index, word in enumerate(vocab, 2)}\n",
    "vocab_to_int['<pad>'] = 0  # The special value used for padding\n",
    "vocab_to_int['<oov>'] = 1  # The special value used for OOVs\n",
    "unique_vocab = len(vocab_to_int)\n",
    "print('Number of unique words:', unique_vocab)\n",
    "print('\\n')\n",
    "\n",
    "# Build labels vocab\n",
    "labels_in_vocab = Counter(labels_tag)\n",
    "labels_vocab = sorted(labels_in_vocab, key=labels_in_vocab.get, reverse=True)\n",
    "label_to_int = {t: i for i, t in enumerate(labels_vocab, 1)}\n",
    "label_to_int['<pad>'] = 0  # The special value used to padding\n",
    "\n",
    "# Check labels\n",
    "no_classes = len(label_to_int)\n",
    "print('Class distribution:', Counter(labels_in_vocab))\n",
    "print('\\n')\n",
    "\n",
    "print('Number of unique labels:', no_classes)\n",
    "print(label_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize input sequences\n",
    "seq_int = []\n",
    "for seq in sequences:\n",
    "    seq_int.append([vocab_to_int[word] for word in seq.split()])\n",
    "\n",
    "# Pad input sequences\n",
    "pad_seq = pad_sequences(sequences=seq_int, maxlen=max_seq_len, padding='post', value=0)\n",
    "\n",
    "# Check sample sequence\n",
    "print('Sample sequence:', sequences[-1])\n",
    "print('\\n')\n",
    "print('Sample sequence:', pad_seq[-1])\n",
    "print('\\n')\n",
    "\n",
    "# Tokenize output labels\n",
    "lab_int = []\n",
    "for lab in y_labels:\n",
    "    lab_int.append([label_to_int[word] for word in lab.split()])\n",
    "\n",
    "# Pad input labels\n",
    "pad_labels = pad_sequences(sequences=lab_int, maxlen=max_seq_len, padding='post', value=0)\n",
    "encoded_labels = [to_categorical(i, num_classes=no_classes) for i in pad_labels]\n",
    "\n",
    "# Check sample label\n",
    "print('Sample label:', pad_labels[-1])\n",
    "print('\\n')\n",
    "print('Encoded label', encoded_labels[-1])\n",
    "\n",
    "# Check max seq length\n",
    "print(\"Maximum sequence length: {}\".format(max_seq_len))\n",
    "\n",
    "# Check that all sequences and labels are at max sequence length \n",
    "assert len(pad_seq)==len(seq_int)\n",
    "assert len(pad_seq[0])==max_seq_len\n",
    "\n",
    "assert len(pad_labels)==len(lab_int)\n",
    "assert len(pad_labels[0])==max_seq_len\n",
    "print('Sequence and labels length check passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and label dataset\n",
    "train_test_split_frac = 0.8\n",
    "split_index = int(0.8*len(pad_seq))\n",
    "\n",
    "# Split data into training, validation, and test data (features and labels, x and y)\n",
    "train_val_x, test_x = pad_seq[:split_index], pad_seq[split_index:]\n",
    "train_val_y, test_y = encoded_labels[:split_index], encoded_labels[split_index:]\n",
    "\n",
    "# print out the shapes of your resultant feature data\n",
    "print('Training/Validation Dataset: \\t{}'.format(train_val_x.shape), len(train_val_y))\n",
    "print('Testing Dataset: \\t\\t{}'.format(test_x.shape), len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad = 0\n",
    "# na = 0\n",
    "# comma = 0\n",
    "# period = 0\n",
    "# qns = 0\n",
    "# excl = 0\n",
    "# dots = 0\n",
    "\n",
    "# for line in test_y:\n",
    "#     for labels in line:\n",
    "#         if np.argmax(labels) == 0:\n",
    "#             pad += 1\n",
    "#         elif np.argmax(labels) == 1:\n",
    "#             na += 1\n",
    "#         elif np.argmax(labels) == 2:\n",
    "#             comma += 1\n",
    "#         elif np.argmax(labels) == 3:\n",
    "#             period += 1\n",
    "#         elif np.argmax(labels) == 4:\n",
    "#             qns += 1\n",
    "#         elif np.argmax(labels) == 5:\n",
    "#             excl += 1\n",
    "#         elif np.argmax(labels) == 6:\n",
    "#             dots += 1\n",
    "\n",
    "# print(pad)\n",
    "# print(na)\n",
    "# print(comma)\n",
    "# print(period)\n",
    "# print(qns)\n",
    "# print(excl)\n",
    "# print(dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glove pre-trained vectors\n",
    "glove_index = dict()\n",
    "f = open('./data/embeddings/glove.6B.300d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove_index[word] = coefs\n",
    "f.close()\n",
    "print('{} word vectors'.format(len(glove_index)))\n",
    "\n",
    "embed_matrix = np.zeros((unique_vocab, embed_dim))\n",
    "for word, i in vocab_to_int.items():\n",
    "    embedding_vector = glove_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embed_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_lstm(max_seq_len, unique_vocab, embed_dim, embed_matrix, filter_sizes, kernels, kernel_weight, bias):\n",
    "    embed_input = Input(shape=(max_seq_len,))\n",
    "\n",
    "    # Add embedding layer using weights from glove\n",
    "    embed = Embedding(input_dim=unique_vocab, output_dim=embed_dim, weights=[embed_matrix], \n",
    "                        input_length=max_seq_len, trainable=True)(embed_input) #104910 * 300\n",
    "    \n",
    "    embed = Dropout(rate=drop_prob, seed=50)(embed)\n",
    "\n",
    "    cnn_outputs = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        # Add conv1d layer\n",
    "        out_i = Conv1D(filters=filter_sizes[i], kernel_initializer=kernel_weight, bias_initializer=bias, \n",
    "                          kernel_size=kernels[i], kernel_regularizer=kernel_reg, activation='relu', \n",
    "                          padding='SAME', strides=1)(embed)\n",
    "        out_i = BatchNormalization()(out_i)\n",
    "        cnn_outputs.append(out_i)\n",
    "\n",
    "    cnn_outputs = concatenate(cnn_outputs, axis=-1)\n",
    "    cnn_outputs = Dropout(rate=drop_prob, seed=50)(cnn_outputs)\n",
    "    cnn_outputs = Reshape((-1, np.sum(filter_sizes)))(cnn_outputs)\n",
    "    \n",
    "    dense = Dense(lstm_hidden, activation='relu')(cnn_outputs)\n",
    "    dense = Dropout(rate=drop_prob, seed=50)(dense)\n",
    "    \n",
    "    blstm_outputs = Bidirectional(LSTM(lstm_hidden_2, return_sequences=True))(dense)\n",
    "    \n",
    "    blstm_outputs = Dropout(rate=drop_prob, seed=50)(blstm_outputs)\n",
    "    \n",
    "    output = TimeDistributed(Dense(no_classes, activation='softmax'))(blstm_outputs)\n",
    "\n",
    "    model = Model(inputs=[embed_input], outputs=[output])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(adam_lr), \n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model code\n",
    "model = cnn_lstm(max_seq_len=max_seq_len, unique_vocab=unique_vocab, embed_dim=embed_dim,\n",
    "                embed_matrix=embed_matrix, filter_sizes=filter_sizes, kernels=kernels,\n",
    "                 kernel_weight=kernel_weight, bias=bias)\n",
    "\n",
    "# Summarize model\n",
    "model.summary()\n",
    "\n",
    "# Fit, train and evaluate model\n",
    "model.fit(x=train_val_x, y=np.array(train_val_y), batch_size=batch_size, \n",
    "          epochs=epochs, validation_split=valid_split, steps_per_epoch=None, validation_steps=None,\n",
    "          shuffle=True, verbose=1, callbacks=[tensor_b, early_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample of test data\n",
    "test_data = test_x[11111]\n",
    "\n",
    "# Restore tokenized test data back to normal sentence\n",
    "pred_x_seq = []\n",
    "for x in test_data:\n",
    "    for value, index in vocab_to_int.items():\n",
    "        if x == index:\n",
    "            pred_x_seq.append(value)\n",
    "\n",
    "# Get predicted output of test data\n",
    "pred_expand = model.predict(np.expand_dims(test_data, axis=0))\n",
    "\n",
    "# Retrieve position of highest probability from prediction\n",
    "pred_y = []\n",
    "for y in pred_expand:\n",
    "    pred_y.append(np.argmax(y, axis=1))\n",
    "print('Predictions Index:')\n",
    "print(pred_y)\n",
    "\n",
    "# Restore tokenized labels\n",
    "pred_y_seq = []\n",
    "for x in pred_y:\n",
    "    for y in x:\n",
    "        for value, index in label_to_int.items():\n",
    "            if y == index:\n",
    "                pred_y_seq.append(value)\n",
    "\n",
    "# Restore punctuations and capitalization                \n",
    "combined = []\n",
    "for i in range(len(pred_x_seq)):\n",
    "    if pred_y_seq[i] == '<comma>':\n",
    "        combined.append(str(pred_x_seq[i])+',')\n",
    "    elif pred_y_seq[i] == '<period>':\n",
    "        combined.append(str(pred_x_seq[i])+'.')\n",
    "    elif pred_y_seq[i] == '<question>':\n",
    "        combined.append(str(pred_x_seq[i])+'?')\n",
    "    elif pred_y_seq[i] == '<exclaim>':\n",
    "        combined.append(str(pred_x_seq[i])+'!')\n",
    "    elif pred_y_seq[i] == '<3-dots>':\n",
    "        combined.append(str(pred_x_seq[i])+'...')\n",
    "    else:\n",
    "        combined.append(str(pred_x_seq[i]))\n",
    "\n",
    "for i in range(len(combined)):\n",
    "    if '.' in combined[i]:\n",
    "        combined[i+1] = combined[i+1].capitalize()\n",
    "    elif combined[i] == 'i':\n",
    "        combined[i] = combined[i].capitalize()\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "combined = ' '.join(combined)\n",
    "combined = combined.replace('<pad>', '')\n",
    "\n",
    "print('\\n')\n",
    "print('Prediction sequence:')            \n",
    "print(' '.join(pred_x_seq))\n",
    "print('\\n')\n",
    "print('Prediction output:')\n",
    "print(' '.join(pred_y_seq))\n",
    "print('\\n')\n",
    "print('Combined prediction:')\n",
    "print(combined.capitalize().replace('ive', \"I've\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix and classification report\n",
    "for_report = model.predict(test_x)\n",
    "out_pred = [np.argmax(x, axis=1) for x in for_report]\n",
    "out_pred = np.concatenate(out_pred, axis=0)\n",
    "\n",
    "y_ = [np.argmax(x, axis=1) for x in test_y]\n",
    "y_ = np.concatenate(y_, axis=0)\n",
    "\n",
    "cm = confusion_matrix(y_true=y_, y_pred=out_pred)\n",
    "print(cm)\n",
    "\n",
    "cr = classification_report(y_true=y_, y_pred=out_pred)\n",
    "print(cr)\n",
    "\n",
    "overall = classification_report(y_true=y_, y_pred=out_pred, output_dict=True)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=class_names, normalize=True, title='Normalized Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comma, period, question, exclaim, 3-dots\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "o_precision = []\n",
    "o_recall = []\n",
    "o_f1 = []\n",
    "\n",
    "for i in range(2, 4):\n",
    "    precision.append(overall[str(i)]['precision'])\n",
    "    recall.append(overall[str(i)]['recall'])\n",
    "    f1.append(overall[str(i)]['f1-score'])\n",
    "\n",
    "precision = np.average(precision)\n",
    "recall = np.average(recall)\n",
    "f1 = np.average(f1)\n",
    "\n",
    "for i in range(2, 7):\n",
    "    o_precision.append(overall[str(i)]['precision'])\n",
    "    o_recall.append(overall[str(i)]['recall'])\n",
    "    o_f1.append(overall[str(i)]['f1-score'])\n",
    "\n",
    "o_precision = np.average(o_precision)\n",
    "o_recall = np.average(o_recall)\n",
    "o_f1 = np.average(o_f1)\n",
    "\n",
    "print('Comma. Period, Question Precision:\\t{:.3f}'.format(precision))\n",
    "print('Comma. Period, Question Recall:\\t\\t{:.3f}'.format(recall))\n",
    "print('Comma. Period, Question F1-Score:\\t{:.3f}'.format(f1))\n",
    "print('\\n')\n",
    "\n",
    "print('Overall Precision:\\t{:.3f}'.format(o_precision))\n",
    "print('Overall Recall:\\t\\t{:.3f}'.format(o_recall))\n",
    "print('Overall F1-Score:\\t{:.3f}'.format(o_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
