{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "[Introduction](#intro)  \n",
    "[Code Flow](#code)  \n",
    "[Model Architecture](#model)  \n",
    "[Results](#results)  \n",
    "[References](#reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='intro'>CNN+BiLSTM approach to Part-of-Speech(PoS) tagging to predict and restore punctuations to sentences</a>\n",
    "\n",
    "The prevalent take on PoS problems have been to use BiLSTM or LSTM models due to their ability to capture and learn dependency information of sentences which are then used to make predictions.\n",
    "\n",
    "This project intends to combine CNN with BiLSTM. Making use of CNNs' ability to capture word and morphological of sentences and forwarding them to the BiLSTM.\n",
    "\n",
    "Outcome is to produce a hybrid model which outperforms a BiLSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name='code'>Code flow</a>\n",
    "\n",
    "General code flow:\n",
    "\n",
    "1. Import packages and dependencies required for the application\n",
    "2. Load and process Ted Talks dataset to generate the corresponding labels  \n",
    "3. Remove punctuations from dataset to remove bias  \n",
    "4. Build vocabulary mappings of the dataset words and labels  \n",
    "5. Tokenize dataset and labels based on mappings\n",
    "6. Pad sequences and labels in dataset to maximum sequence length\n",
    "7. One-hot encode labels as data is not ordinal\n",
    "8. Split dataset into training and testing sets\n",
    "9. Construct hybrid model\n",
    "10. Feed training data into model with 30% of it used as validation\n",
    "11. Load and process Glove embeddings to extract weight matrix based on unique words in dataset\n",
    "12. Fit and train model with early stopping and checkpoint save of best results\n",
    "13. Save image of model architecture\n",
    "14. Delete current model and load saved model with best results\n",
    "15. Retrieve a sample of testing data and make a prediction on it\n",
    "16. Restore and print out results of prediction\n",
    "17. Make predictions using testing set and construct a Confusion Matrix & Classification Report from it to evaluate model\n",
    "18. Print out Precision, Recall and F1-Scores for testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "from numpy.random import seed\n",
    "seed(50)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(50)\n",
    "\n",
    "# Import required packages and dependencies\n",
    "import io, json, keras, string, itertools, random, time, datetime, numpy as np, matplotlib.pyplot as plt, tensorflow as tf\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import glorot_uniform, random_uniform\n",
    "from keras.layers import Activation\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Embedding, Conv1D, Flatten, Dense, Dropout, LSTM, Bidirectional, TimeDistributed, \\\n",
    "Dropout, Input, concatenate, Reshape\n",
    "from keras import regularizers\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom functions to help display the Confusion Matrix and process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom functions\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    '''\n",
    "    Description: \n",
    "        - Prints and plots the confusion matrix.\tNormalization can be applied by setting `normalize=True`\n",
    "\n",
    "    Args:\n",
    "        - cm: Confusion Matrix\n",
    "        - classes: Names of classes\n",
    "        - normalize: Whether to or to not normal values in Confusion Matrix\n",
    "        - cmap: Plot color\n",
    "    '''\n",
    "\n",
    "    # Check if normalize is true or false\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    # Format axis and plot Confusion Matrix\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "def get_labels(seq):\n",
    "    '''\n",
    "    Description: \n",
    "        - Creates a sequence of labels based on the input sequence\n",
    "\n",
    "    Args:\n",
    "        - seq: Input sequence\n",
    "    \n",
    "    Returns:\n",
    "        - Sequence labels\n",
    "    '''\n",
    "    \n",
    "    labels_seq = []\n",
    "    seq = seq.split()\n",
    "    for i in range(len(seq)):\n",
    "        if '...' in seq[i]:\n",
    "            labels_seq.append('<3-dots>')\n",
    "        elif ',' in seq[i]:\n",
    "            labels_seq.append('<comma>')\n",
    "        elif '.' in seq[i]:\n",
    "            labels_seq.append('<period>')\n",
    "        elif '?' in seq[i]:\n",
    "            labels_seq.append('<question>')\n",
    "        elif '!' in seq[i]:\n",
    "            labels_seq.append('<exclaim>')\n",
    "        else:\n",
    "            labels_seq.append('<na>')\n",
    "    return labels_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model parameters\n",
    "model_name = 'ted-glove-cnn-lstm'\n",
    "\n",
    "# Dimension of the embedding layer, must match that of the word vectors\n",
    "embed_dim = 300\n",
    "\n",
    "# Maximum sequence length, how long each sentence/sequence should be\n",
    "max_seq_len = 128\n",
    "\n",
    "# Dropout are\n",
    "drop_prob = 0.35\n",
    "\n",
    "# Number of filters for each CNN layer\n",
    "filter_sizes = [64,64,64]\n",
    "\n",
    "# Kernel size for each CNN layer\n",
    "kernels = [3,5,7]\n",
    "\n",
    "# Weights and bias initialization for each CNN layer\n",
    "kernel_weight = glorot_uniform()\n",
    "bias = glorot_uniform()\n",
    "\n",
    "# Regularization for each CNN layer\n",
    "kernel_reg = regularizers.l2(l=0.0001)\n",
    "\n",
    "# Number of hidden units for Dense layer\n",
    "lstm_hidden = 1024\n",
    "\n",
    "# Number of hidden units for BiLSTM layer\n",
    "lstm_hidden_2 = 1024\n",
    "\n",
    "# Learning rate for Adam optimizer\n",
    "adam_lr = 0.001\n",
    "\n",
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Number of epochs to train for\n",
    "epochs = 50\n",
    "\n",
    "# Portion of training data to be used for validation\n",
    "valid_split = 0.3\n",
    "\n",
    "# Set model training early stop criteria and save best checkpoint file\n",
    "early_s = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "chkpt = ModelCheckpoint(filepath='./cnn_lstm_model.h5', monitor='val_loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define miscellaneous settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set misc parameters\n",
    "# Get current date and time\n",
    "current = datetime.datetime.now()\n",
    "date = current.strftime('%b-%d')\n",
    "\n",
    "# Tensorboard settings\n",
    "tensor_b = TensorBoard(log_dir='./tf_logs/model_{}_hidden_{}_dropout_{}_embed_dim_{}_lr_{}'.format(model_name, \n",
    "                        lstm_hidden, drop_prob,\n",
    "                        embed_dim, adam_lr), \n",
    "                        batch_size=batch_size, \n",
    "                        write_graph=True, histogram_freq=0)\n",
    "\n",
    "# Set class names\n",
    "class_names = ['Pad', 'NA', 'Comma', 'Period', 'Question', 'Exclaim', '3-Dots']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and process Ted Talks dataset to generate the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre number of sentences: 237986\n",
      "\n",
      "\n",
      "(185073, '  and this is more fun.so this last one is called \"the sunshine kid.\"thank you very much for listening.old man sunshine was proud of his sun,and it brightened his day to see his little boy run,not because of what he’d done, nor the problems overcome,but that despite that his disposition remained a sunny one.it hadn’t always been like this.there’d been times when he’d tried to hide his brightness,you see, every star hits periods of hardship,it takes a brighter light to inspire them through the darkness.if we go back to when he was born in a nebula,we know that he never was thought of as regular,because he had a flair about him,to say the midas touch is wrongbut all he went near seemed to turn a little bronze,yes this sun was loved by some more than others,it was a case of joseph and his dreamcoat and his brothersbecause standing out from the crowd had its pros and its cons,and jealousy created enemies in those he outshonesuch as the shadow people.now the shadow people didn’t like the sunshine kid,because he showed up the dark things the shadow people did,and when he shone he showed the places where the shadow people hid,so the shadow people had an evil plan to get rid of him,first up — they made fun of his sunspots,shooting his dreams from the sky, their words were gunshots,designed to remind him he wasn’t very cooland he didn’t fit in with any popular kids at school.they said his head was up in space and they would bring him down to earth,essentially he came from nothing and that is what he was worth,he’d never get to go to university to learn,only degrees he’d ever show would be the first degree burnsfrom those that came too close, they told him he was too bright,that’s why no one ever looked him in the eyes,his judgment became cloudedso did the sky, with evaporated tearsas the sun started to cry.because the sunshine kid was bright, with a warm personality,and inside he burned savagelyhurt by the words and curses of the shadowy folkwho spoke holes in his soul and left cavities,and as his heart hardened, his spark darkened,every time they called him names it cooled his flames,he thought they might like him if he kept his light dimbut they were busy telling lightning she had terrible aim,he couldn’t quite get to grips with what they said,so he let his light be eclipsed by what they said,he fell into a lone star state like texas,and felt like he’d been punched in his solar plexus.but that’s when little miss sunshine came alongsinging her favorite song about how we’re made to be strong,and you don’t have to be wrong to belong, just be true to who you are,because we are all stars at heart.little miss sunshine was hot stuff,the kind of girl when you looked at heryou forgot stuff,but for him, there was no forgetting her,the minute he saw her her image burned in his retina,she was out of this world, and she accepted him,something about this girl meant he knew whenever she was next to him,things weren’t as dark as they seemed, and he dared to dream,shadows were nowhere to be seen; when she was there he beamed,his eyes would light up in ways that can’t be faked,when she grinned her rays erased the razor-tipped words of hate,they gave each other nicknames, they were \"cool star\" and \"fun sun,\"and gradually the shadowy damage became undone,she was one in a septillion, and she was brilliant,could turn the coldest blooded reptilians vermillion,loved by billions, from chileans to brazilians,and taught the sunshine kid the meaning of resilience.she said: “all the darkness in the worldcannot put out the light from a single candleso how the hell can they handle your light?only you can choose to dim it, and the sky is the limit, so silence the critics by burning.”and if eyes are windows to the soul then she drew back the curtainsand let the sun shine through the hurting.in a universe of adversity these stars stuck together,and though days became nights the memories would last forever,whether the weatherman said it or not, it would be fine,\\'cause even behind the clouds the kid could still shine.yes, the sunshine kid was bright, with a warm personality,and inside he burned savagely,fueled by the fire inspired across galaxiesby the girl who showed him belief.thank you very much. twenty-five years ago, scientists at cern created the world wide web.')\n",
      "\n",
      "\n",
      "Length of longest sentence: 4305\n",
      "Chunked longest sentence: 19\n",
      "Post number of sentences: 238004\n",
      "\n",
      "\n",
      "Last Sentence  twenty-five years ago,  scientists at cern created the world wide web.\n",
      "\n",
      "\n",
      "Counter({' ': 5183083, 'e': 2732494, 't': 2236140, 'a': 1861243, 'o': 1797491, 'i': 1648311, 'n': 1556267, 's': 1429598, 'r': 1235404, 'h': 1200803, 'l': 926456, 'd': 817246, 'u': 676726, 'c': 626734, 'm': 542737, 'w': 529153, 'y': 504344, 'g': 480783, 'f': 439860, 'p': 413986, 'b': 347437, 'v': 239213, 'k': 204683, 'x': 40701, 'j': 37556, '0': 34453, '—': 27887, 'z': 21725, 'q': 18177, '1': 16995, '2': 11304, '5': 7462, '9': 6382, '3': 5412, '4': 4555, '8': 3904, '6': 3514, '7': 3450, '’': 920, '£': 291, 'é': 283, 'í': 84, 'á': 65, 'ó': 48, 'ç': 36, 'ã': 35, 'è': 30, 'ö': 29, '“': 21, 'ñ': 20, '”': 20, 'ï': 17, 'ü': 13, 'à': 12, 'ù': 11, 'ā': 10, 'ä': 7, 'ø': 7, 'ê': 6, '\\xa0': 6, '‘': 6, 'â': 5, 'ō': 5, 'อ': 4, 'ë': 3, 'ô': 3, '²': 3, '\\x80': 3, 'ī': 3, 'ì': 3, 'ʾ': 3, 'ć': 2, 'ร': 2, '่': 2, 'ย': 2, 'æ': 2, '\\x93': 2, '•': 2, 'û': 1, 'º': 1, '˚': 1, 'ò': 1, '送': 1, '你': 1, '葱': 1, '¢': 1, '\\x94': 1, 'ă': 1, 'ť': 1, '€': 1, '∇': 1, 'τ': 1, 'č': 1, '¡': 1, 'å': 1, 'ě': 1, 'ū': 1, '¿': 1, 'ú': 1, 'ð': 1, 'प': 1, '्': 1, 'र': 1, 'े': 1, 'म': 1, 'š': 1, 'ọ': 1, '̀': 1, 'ẹ': 1})\n",
      "\n",
      "\n",
      "Number of sequences: \t238003\n",
      "Number of labels: \t238003\n"
     ]
    }
   ],
   "source": [
    "# Read and load dataset\n",
    "data = open('./data/processed/ted_data', 'r', encoding='utf-8').read()\n",
    "\n",
    "# Convert all characters to lowercase\n",
    "data = data.lower()\n",
    "\n",
    "# Look-up table to remove punctuations from data\n",
    "table = str.maketrans('', '', punctuation)\n",
    "\n",
    "# Define and remove characters and bracketed actions\n",
    "replace = ['♫', '♪', '–', '…', '(applause)', '(laughter)']\n",
    "for i in range(len(replace)):\n",
    "    data = data.replace(replace[i], ' ')\n",
    "\n",
    "# Split dataset by sentences\n",
    "data_split = data.split('\\n')\n",
    "print('Pre number of sentences:', len(data_split))\n",
    "print('\\n')\n",
    "# Get longest sentence in dataset and its index\n",
    "print(max(enumerate(data_split), key=lambda x: len(x[1])))\n",
    "print('\\n')\n",
    "print('Length of longest sentence:', len(max(data_split, key=len)))\n",
    "\n",
    "# Clean and split the longest sentence into multiple ones based on full-stops\n",
    "data_split[185073] = data_split[185073].replace(',', ', ')\n",
    "data_split[185073] = data_split[185073].replace('.', '.\\n')\n",
    "long_sent = data_split[185073].split('\\n')\n",
    "\n",
    "# Check number of sentences from chunking longest sentence\n",
    "print('Chunked longest sentence:', len(long_sent))\n",
    "\n",
    "# Remove longest sentence at index 185703\n",
    "del data_split[185073]\n",
    "\n",
    "# Add chunked sentences back to dataset\n",
    "for x in long_sent:\n",
    "    data_split.append(x)\n",
    "\n",
    "# Check length of dataset after addition\n",
    "print('Post number of sentences:', len(data_split))\n",
    "print('\\n')\n",
    "\n",
    "# Remove empty rows\n",
    "data_split = data_split[:238003]\n",
    "\n",
    "# Check last sentence of dataset\n",
    "print('Last Sentence', data_split[-1])\n",
    "print('\\n')\n",
    "\n",
    "# Get corresponding labels for dataset\n",
    "process_labels = [get_labels(seq) for seq in data_split]\n",
    "process_labels = [' '.join(seq) for seq in process_labels]\n",
    "\n",
    "# Remove all punctuations from dataset\n",
    "sequences = [seq.translate(table) for seq in data_split]\n",
    "\n",
    "# Combined sentences back into a single piece for Counter\n",
    "combined_sequences = ' '.join(sequences)\n",
    "\n",
    "# Check if there are additional characters to remove\n",
    "print(Counter(combined_sequences))\n",
    "print('\\n')\n",
    "    \n",
    "# Get all words in the dataset\n",
    "words = combined_sequences.split()\n",
    "\n",
    "# Save inputs and labels for reference\n",
    "with open('./data/processed/processed_input', 'w', encoding='utf-8') as f:\n",
    "    for x in sequences:\n",
    "        f.write(x+'\\n')\n",
    "with open('./data/processed/processed_labels', 'w', encoding='utf-8') as f:\n",
    "    for x in process_labels:\n",
    "        f.write(x+'\\n')\n",
    "\n",
    "# Check number of sequences and labels\n",
    "print('Number of sequences: \\t{}'.format(len(sequences)))\n",
    "print('Number of labels: \\t{}'.format(len(process_labels)))\n",
    "\n",
    "# Load processed labels\n",
    "y_labels = open('./data/processed/processed_labels', 'r', encoding='utf-8').read()\n",
    "y_labels = y_labels.split('\\n')\n",
    "y_labels = y_labels[:-1]\n",
    "all_labels = ' '.join(y_labels)\n",
    "\n",
    "# Get all labels in the dataset\n",
    "labels_tag = all_labels.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build words and labels vocabularies and store them as json dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 104910\n",
      "\n",
      "\n",
      "Class distribution: Counter({'<na>': 4475054, '<comma>': 360733, '<period>': 294389, '<question>': 26054, '<exclaim>': 2330, '<3-dots>': 1394})\n",
      "\n",
      "\n",
      "Number of unique labels: 7\n",
      "{'<na>': 1, '<comma>': 2, '<period>': 3, '<question>': 4, '<exclaim>': 5, '<3-dots>': 6, '<pad>': 0}\n"
     ]
    }
   ],
   "source": [
    "# Build words vocab\n",
    "all_data = ' '.join(sequences)\n",
    "words = all_data.split()\n",
    "words_in_vocab = Counter(words)\n",
    "vocab = sorted(words_in_vocab, key=words_in_vocab.get, reverse=True)\n",
    "\n",
    "# Skip most common word\n",
    "vocab_to_int = {word: index for index, word in enumerate(vocab, 2)}\n",
    "vocab_to_int['<pad>'] = 0  # The special value used for padding\n",
    "vocab_to_int['<oov>'] = 1  # The special value used for OOVs\n",
    "\n",
    "# Check number of unique words\n",
    "unique_vocab = len(vocab_to_int)\n",
    "print('Number of unique words:', unique_vocab)\n",
    "print('\\n')\n",
    "\n",
    "# Build labels vocab\n",
    "labels_in_vocab = Counter(labels_tag)\n",
    "labels_vocab = sorted(labels_in_vocab, key=labels_in_vocab.get, reverse=True)\n",
    "label_to_int = {t: i for i, t in enumerate(labels_vocab, 1)}\n",
    "label_to_int['<pad>'] = 0  # The special value used to padding\n",
    "\n",
    "# Write vocab and label dictionaries to file\n",
    "with open('./vocabs.json', 'w', encoding='utf-8') as fv:\n",
    "    json.dump(vocab_to_int, fv, indent=4)\n",
    "    \n",
    "with open('./labels.json', 'w', encoding='utf-8') as fl:\n",
    "    json.dump(label_to_int, fl, indent=4)\n",
    "    \n",
    "# Check label classes distribution\n",
    "no_classes = len(label_to_int)\n",
    "print('Class distribution:', Counter(labels_in_vocab))\n",
    "print('\\n')\n",
    "\n",
    "# Check number of unique labels\n",
    "print('Number of unique labels:', no_classes)\n",
    "print(label_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the sequences and their corresponding labels. Pad each sequence and its labels to maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sequence:  twentyfive years ago  scientists at cern created the world wide web\n",
      "\n",
      "\n",
      "Sample sequence: [14518    84   197   649    31 10130   501     2    81  1929   948     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "\n",
      "Sample label: [1 1 2 1 1 1 1 1 1 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "\n",
      "Encoded label [[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n",
      "Maximum sequence length: 128\n",
      "Sequence and labels length check passed!\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input sequences\n",
    "seq_int = []\n",
    "for seq in sequences:\n",
    "    seq_int.append([vocab_to_int[word] for word in seq.split()])\n",
    "\n",
    "# Pad input sequences\n",
    "pad_seq = pad_sequences(sequences=seq_int, maxlen=max_seq_len, padding='post', value=0)\n",
    "\n",
    "# Check sample sequence\n",
    "print('Sample sequence:', sequences[-1])\n",
    "print('\\n')\n",
    "print('Sample sequence:', pad_seq[-1])\n",
    "print('\\n')\n",
    "\n",
    "# Tokenize output labels\n",
    "lab_int = []\n",
    "for lab in y_labels:\n",
    "    lab_int.append([label_to_int[word] for word in lab.split()])\n",
    "\n",
    "# Pad input labels\n",
    "pad_labels = pad_sequences(sequences=lab_int, maxlen=max_seq_len, padding='post', value=0)\n",
    "encoded_labels = [to_categorical(i, num_classes=no_classes) for i in pad_labels]\n",
    "\n",
    "# Check sample label\n",
    "print('Sample label:', pad_labels[-1])\n",
    "print('\\n')\n",
    "print('Encoded label', encoded_labels[-1])\n",
    "\n",
    "# Check max seq length\n",
    "print(\"Maximum sequence length: {}\".format(max_seq_len))\n",
    "\n",
    "# Check that all sequences and labels are at max sequence length \n",
    "assert len(pad_seq)==len(seq_int)\n",
    "assert len(pad_seq[0])==max_seq_len\n",
    "\n",
    "assert len(pad_labels)==len(lab_int)\n",
    "assert len(pad_labels[0])==max_seq_len\n",
    "print('Sequence and labels length check passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Dataset: \t(190402, 128) 190402\n",
      "Testing Dataset: \t\t(47601, 128) 47601\n"
     ]
    }
   ],
   "source": [
    "# Split train and label dataset\n",
    "train_test_split_frac = 0.8\n",
    "split_index = int(0.8*len(pad_seq))\n",
    "\n",
    "# Split data into training, validation, and test data (features and labels, x and y)\n",
    "train_val_x, test_x = pad_seq[:split_index], pad_seq[split_index:]\n",
    "train_val_y, test_y = encoded_labels[:split_index], encoded_labels[split_index:]\n",
    "\n",
    "# print out the shapes of your resultant feature data\n",
    "print('Training/Validation Dataset: \\t{}'.format(train_val_x.shape), len(train_val_y))\n",
    "print('Testing Dataset: \\t\\t{}'.format(test_x.shape), len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and process Glove pretrained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 word vectors\n"
     ]
    }
   ],
   "source": [
    "# Load glove pre-trained vectors\n",
    "glove_index = dict()\n",
    "f = open('./data/embeddings/glove.6B.300d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove_index[word] = coefs\n",
    "f.close()\n",
    "print('{} word vectors'.format(len(glove_index)))\n",
    "\n",
    "embed_matrix = np.zeros((unique_vocab, embed_dim))\n",
    "for word, i in vocab_to_int.items():\n",
    "    embedding_vector = glove_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embed_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name='model'> Model Architecture </a>\n",
    "\n",
    "The model consists of a few parts:\n",
    "\n",
    "1. Embedding Layer:  \n",
    "    * While it is possible to initialize and use a new empty embedding for the network to train and learn on, using a pre-trained embedding improves the model performance\n",
    "    *  Matrix of weights extracted from the Glove pre-trained word vectors based on the number of unique words in the dataset\n",
    "    \n",
    "    \n",
    "2. CNN Layers:\n",
    "    * For each sequence in the dataset, a convolution window of x (Where x is the kernel size) is applied, capturing the morphological and feature information of x words at a time\n",
    "    * 3 CNN layers each generating 128 filters with 3, 5 and 7 kernel sizes. \n",
    "    * Stride 1 is used and padding is applied (SAME) to keep the output length same as the input.\n",
    "    * Outputs from the CNN layers are then concatenated and reshaped\n",
    "    * They are then fed through a dense layer with ReLU activation to learn about their non-linear relationships \n",
    "\n",
    "\n",
    "3. BiLSTM Layer\n",
    "    * Outputs from dense layer are then fed into a BiLSTM layer to learn about the structure and dependencies of the sequences\n",
    "    *  A final Time Distributed dense layer is applied to make predictions on each word in the sequences\n",
    "    \n",
    "![Model Architecture](./images/model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_lstm(max_seq_len, unique_vocab, embed_dim, embed_matrix, filter_sizes, kernels, kernel_weight, bias):\n",
    "    '''\n",
    "    Description: \n",
    "        - Constructs and compiles the CNN+BiLSTM model\n",
    "    \n",
    "    Args(They can be defined earlier at the top of the notebook):\n",
    "        - max_seq_len: Maximum sequence length \n",
    "        - unique_vocab: Number of unique words\n",
    "        - embed_dim: Embedding layer dimension, needs to match with that of Glove pre-trained\n",
    "        - embed_matrix: Pre-trained weights extracted from Glove based on unique words\n",
    "        - filter_sizes: Number of filters per CNN layer\n",
    "        - kernels: Kernel sizes per CNN layer\n",
    "        - kernel_weight: Weights initialization for CNN layers\n",
    "        - bias: Bias initialization for CNN layers\n",
    "        \n",
    "    Return: \n",
    "        - Compiled model\n",
    "    '''\n",
    "    embed_input = Input(shape=(max_seq_len,))\n",
    "\n",
    "    # Add embedding layer using weights from glove\n",
    "    embed = Embedding(input_dim=unique_vocab, output_dim=embed_dim, weights=[embed_matrix], \n",
    "                        input_length=max_seq_len, trainable=False)(embed_input) #104910 * 300\n",
    "    \n",
    "    embed = Dropout(rate=drop_prob)(embed)\n",
    "\n",
    "    cnn_outputs = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        # Add conv1d layer\n",
    "        out_i = Conv1D(filters=filter_sizes[i], kernel_initializer=kernel_weight, bias_initializer=bias, \n",
    "                          kernel_size=kernels[i], kernel_regularizer=None, activation='relu', \n",
    "                          padding='SAME', strides=1)(embed)\n",
    "#         out_i = BatchNormalization()(out_i)\n",
    "        cnn_outputs.append(out_i)\n",
    "\n",
    "    cnn_outputs = concatenate(cnn_outputs, axis=-1)\n",
    "    cnn_outputs = Dropout(rate=drop_prob)(cnn_outputs)\n",
    "    cnn_outputs = Reshape((-1, np.sum(filter_sizes)))(cnn_outputs)\n",
    "    \n",
    "    dense = Dense(lstm_hidden, activation='relu')(cnn_outputs)\n",
    "    dense = Dropout(rate=drop_prob)(dense)\n",
    "    \n",
    "    blstm_outputs = Bidirectional(LSTM(lstm_hidden_2, return_sequences=True))(dense)\n",
    "    \n",
    "    blstm_outputs = Dropout(rate=drop_prob)(blstm_outputs)\n",
    "    \n",
    "    output = TimeDistributed(Dense(no_classes, activation='softmax'))(blstm_outputs)\n",
    "\n",
    "    model = Model(inputs=[embed_input], outputs=[output])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(adam_lr), \n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 128, 300)     31473000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 128, 64)      57664       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 128, 64)      96064       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 128, 64)      134464      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 192)     0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128, 192)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 128, 192)     0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128, 1024)    197632      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128, 1024)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128, 2048)    16785408    dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128, 2048)    0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 128, 7)       14343       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 48,758,575\n",
      "Trainable params: 17,285,575\n",
      "Non-trainable params: 31,473,000\n",
      "__________________________________________________________________________________________________\n",
      "Train on 133281 samples, validate on 57121 samples\n",
      "Epoch 1/50\n",
      "133281/133281 [==============================] - 1232s 9ms/step - loss: 0.0401 - acc: 0.9871 - val_loss: 0.0332 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03323, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 2/50\n",
      "133281/133281 [==============================] - 1237s 9ms/step - loss: 0.0310 - acc: 0.9893 - val_loss: 0.0293 - val_acc: 0.9898\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03323 to 0.02934, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 3/50\n",
      "133281/133281 [==============================] - 1240s 9ms/step - loss: 0.0284 - acc: 0.9901 - val_loss: 0.0277 - val_acc: 0.9902\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02934 to 0.02768, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 4/50\n",
      "133281/133281 [==============================] - 1239s 9ms/step - loss: 0.0269 - acc: 0.9906 - val_loss: 0.0265 - val_acc: 0.9906\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02768 to 0.02648, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 5/50\n",
      "133281/133281 [==============================] - 1239s 9ms/step - loss: 0.0259 - acc: 0.9909 - val_loss: 0.0257 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02648 to 0.02567, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 6/50\n",
      "133281/133281 [==============================] - 1239s 9ms/step - loss: 0.0251 - acc: 0.9911 - val_loss: 0.0251 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02567 to 0.02513, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 7/50\n",
      "133281/133281 [==============================] - 1238s 9ms/step - loss: 0.0245 - acc: 0.9913 - val_loss: 0.0245 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02513 to 0.02453, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 8/50\n",
      "133281/133281 [==============================] - 1240s 9ms/step - loss: 0.0241 - acc: 0.9914 - val_loss: 0.0243 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02453 to 0.02429, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 9/50\n",
      "133281/133281 [==============================] - 1238s 9ms/step - loss: 0.0237 - acc: 0.9915 - val_loss: 0.0241 - val_acc: 0.9914\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02429 to 0.02412, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 10/50\n",
      "133281/133281 [==============================] - 1239s 9ms/step - loss: 0.0234 - acc: 0.9916 - val_loss: 0.0239 - val_acc: 0.9914\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02412 to 0.02394, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 11/50\n",
      "133281/133281 [==============================] - 1238s 9ms/step - loss: 0.0231 - acc: 0.9917 - val_loss: 0.0237 - val_acc: 0.9916\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02394 to 0.02372, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 12/50\n",
      "133281/133281 [==============================] - 1236s 9ms/step - loss: 0.0229 - acc: 0.9917 - val_loss: 0.0238 - val_acc: 0.9915\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02372\n",
      "Epoch 13/50\n",
      "133281/133281 [==============================] - 1239s 9ms/step - loss: 0.0226 - acc: 0.9918 - val_loss: 0.0235 - val_acc: 0.9916\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02372 to 0.02347, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 14/50\n",
      "133281/133281 [==============================] - 1238s 9ms/step - loss: 0.0225 - acc: 0.9918 - val_loss: 0.0234 - val_acc: 0.9916\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02347 to 0.02338, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 15/50\n",
      "133281/133281 [==============================] - 1235s 9ms/step - loss: 0.0223 - acc: 0.9919 - val_loss: 0.0234 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02338\n",
      "Epoch 16/50\n",
      "133281/133281 [==============================] - 1235s 9ms/step - loss: 0.0222 - acc: 0.9919 - val_loss: 0.0233 - val_acc: 0.9916\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02338 to 0.02335, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 17/50\n",
      "133281/133281 [==============================] - 1250s 9ms/step - loss: 0.0220 - acc: 0.9920 - val_loss: 0.0233 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02335 to 0.02325, saving model to ./cnn_lstm_model.h5\n",
      "Epoch 18/50\n",
      "133281/133281 [==============================] - 1280s 10ms/step - loss: 0.0219 - acc: 0.9920 - val_loss: 0.0234 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02325\n",
      "Epoch 19/50\n",
      "  7936/133281 [>.............................] - ETA: 17:41 - loss: 0.0213 - acc: 0.9922"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b3ca354347c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m model.fit(x=train_val_x, y=np.array(train_val_y), batch_size=batch_size, \n\u001b[0;32m     12\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m           shuffle=True, verbose=1, callbacks=[tensor_b, early_s, chkpt])\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time taken: {} seconds'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model code\n",
    "model = cnn_lstm(max_seq_len=max_seq_len, unique_vocab=unique_vocab, embed_dim=embed_dim,\n",
    "                embed_matrix=embed_matrix, filter_sizes=filter_sizes, kernels=kernels,\n",
    "                 kernel_weight=kernel_weight, bias=bias)\n",
    "\n",
    "# Summarize model\n",
    "model.summary()\n",
    "\n",
    "# Fit, train and evaluate model\n",
    "start = time.time()\n",
    "model.fit(x=train_val_x, y=np.array(train_val_y), batch_size=batch_size, \n",
    "          epochs=epochs, validation_split=valid_split, steps_per_epoch=None, validation_steps=None,\n",
    "          shuffle=True, verbose=1, callbacks=[tensor_b, early_s, chkpt])\n",
    "print('Time taken: {} seconds'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model architecture and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete previous model and load model with best results\n",
    "del model\n",
    "model = load_model('./cnn_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a sample test data, make a prediction from it and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Index:\n",
      "[array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3,\n",
      "       3, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)]\n",
      "\n",
      "\n",
      "Prediction sequence:\n",
      "this is where the robots and models that ive presented today will hopefully play a key role towards these very important goalsthank you bruno giussani auke ive seen in your lab other robots that do things like swim in pollution and measure the pollution while they swim <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "\n",
      "Prediction output:\n",
      "<na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <period> <period> <na> <na> <comma> <na> <na> <na> <na> <comma> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <period> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "\n",
      "Combined prediction:\n",
      "This is where the robots and models that I've presented today will hopefully play a key role towards these very important goalsthank. you. bruno giussani auke, I've seen in your lab, other robots that do things like swim in pollution and measure the pollution while they swim.                                                                                 \n"
     ]
    }
   ],
   "source": [
    "# Load a sample of test data\n",
    "test_data = test_x[11111]\n",
    "\n",
    "# Restore tokenized test data back to normal sentence\n",
    "pred_x_seq = []\n",
    "for x in test_data:\n",
    "    for value, index in vocab_to_int.items():\n",
    "        if x == index:\n",
    "            pred_x_seq.append(value)\n",
    "\n",
    "# Get predicted output of test data (Make predictions)\n",
    "pred_expand = model.predict(np.expand_dims(test_data, axis=0))\n",
    "\n",
    "# Retrieve position of highest probability from predictions\n",
    "pred_y = []\n",
    "for y in pred_expand:\n",
    "    pred_y.append(np.argmax(y, axis=1))\n",
    "print('Predictions Index:')\n",
    "print(pred_y)\n",
    "\n",
    "# Restore tokenized labels\n",
    "pred_y_seq = []\n",
    "for x in pred_y:\n",
    "    for y in x:\n",
    "        for value, index in label_to_int.items():\n",
    "            if y == index:\n",
    "                pred_y_seq.append(value)\n",
    "\n",
    "# Restore punctuations and capitalization                \n",
    "combined = []\n",
    "for i in range(len(pred_x_seq)):\n",
    "    if pred_y_seq[i] == '<comma>':\n",
    "        combined.append(str(pred_x_seq[i])+',')\n",
    "    elif pred_y_seq[i] == '<period>':\n",
    "        combined.append(str(pred_x_seq[i])+'.')\n",
    "    elif pred_y_seq[i] == '<question>':\n",
    "        combined.append(str(pred_x_seq[i])+'?')\n",
    "    elif pred_y_seq[i] == '<exclaim>':\n",
    "        combined.append(str(pred_x_seq[i])+'!')\n",
    "    elif pred_y_seq[i] == '<3-dots>':\n",
    "        combined.append(str(pred_x_seq[i])+'...')\n",
    "    else:\n",
    "        combined.append(str(pred_x_seq[i]))\n",
    "\n",
    "for i in range(len(combined)):\n",
    "    if '.' in combined[i]:\n",
    "        combined[i+1] = combined[i+1].capitalize()\n",
    "    elif combined[i] == 'i':\n",
    "        combined[i] = combined[i].capitalize()\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Join predicted words back into a sequence\n",
    "combined = ' '.join(combined)\n",
    "combined = combined.replace('<pad>', '')\n",
    "\n",
    "print('\\n')\n",
    "print('Prediction sequence:')            \n",
    "print(' '.join(pred_x_seq))\n",
    "print('\\n')\n",
    "print('Prediction output:')\n",
    "print(' '.join(pred_y_seq))\n",
    "print('\\n')\n",
    "print('Combined prediction:')\n",
    "print(combined.capitalize().replace('ive', \"I've\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name='results'> Results </a>\n",
    "Barring the Pad and NA classes which are not our targets, the below can be observed from the Classifcation Report and Confusion Matrix:\n",
    "\n",
    "* Model performs decently on predicting Commas, Periods and Question Marks achieving 61%, 89% and, 63% F1-Score respectively\n",
    "* Predictions for Exclaimation Marks and 3 Consecutive Dots performed the worst at 7% and 9% F1-Score respectively\n",
    "* The difference in performance could possibly be due to class imbalances resulting in the model \"viewing\" insufficient exmaples to make more accurate predictions \n",
    "\n",
    "##### Ted Talks vs Baseline vs MGE\n",
    "![Comparison](./images/comparision2.jpg)\n",
    "\n",
    "When compared \n",
    "\n",
    "##### Parameters, model explorations and improvements\n",
    "\n",
    "Mutiple model parameters have been adjusted in hopes of improving the performance. Their results can be viewed [here](https://docs.google.com/spreadsheets/d/1FguxMPOCumn6J5Zgo7-S6yhNAFMkLuPKATyPfGzwkq8/edit#gid=301828972)\n",
    "\n",
    "The following model configurations have also been attempted:\n",
    "* Increasing the amount of CNN layers to 4: No substantial improvements\n",
    "* Concatenating the outputs of the CNN layers instead of passing them through each layer: Model performance improves\n",
    "\n",
    "##### Further explorations\n",
    "* Increase dataset size and monitor if there are improvements\n",
    "* Add attention mechanism to track critical outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Confusion Matrix and Classification Report to check model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4931197       0       0      34       0       0       0]\n",
      " [    329  993493   17011    3394     647      22       9]\n",
      " [     18   28871   44674    1833     682      10       1]\n",
      " [    617    4321    3494   55539     157       8       3]\n",
      " [      9    1060     673     265    3513       2       0]\n",
      " [      5     197     228     106      12      18       0]\n",
      " [    305      67      15      61       2       0      26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   4931231\n",
      "           1       0.97      0.98      0.97   1014905\n",
      "           2       0.68      0.59      0.63     76089\n",
      "           3       0.91      0.87      0.89     64139\n",
      "           4       0.70      0.64      0.67      5522\n",
      "           5       0.30      0.03      0.06       566\n",
      "           6       0.67      0.05      0.10       476\n",
      "\n",
      "   micro avg       0.99      0.99      0.99   6092928\n",
      "   macro avg       0.75      0.59      0.62   6092928\n",
      "weighted avg       0.99      0.99      0.99   6092928\n",
      "\n",
      "Normalized confusion matrix\n",
      "[[9.99993105e-01 0.00000000e+00 0.00000000e+00 6.89483011e-06\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.24168272e-04 9.78902459e-01 1.67611747e-02 3.34415536e-03\n",
      "  6.37498091e-04 2.16769057e-05 8.86782507e-06]\n",
      " [2.36565075e-04 3.79437238e-01 5.87128231e-01 2.40902101e-02\n",
      "  8.96318785e-03 1.31425042e-04 1.31425042e-05]\n",
      " [9.61973214e-03 6.73693073e-02 5.44754362e-02 8.65916213e-01\n",
      "  2.44780867e-03 1.24729104e-04 4.67734140e-05]\n",
      " [1.62984426e-03 1.91959435e-01 1.21876132e-01 4.79898587e-02\n",
      "  6.36182543e-01 3.62187613e-04 0.00000000e+00]\n",
      " [8.83392226e-03 3.48056537e-01 4.02826855e-01 1.87279152e-01\n",
      "  2.12014134e-02 3.18021201e-02 0.00000000e+00]\n",
      " [6.40756303e-01 1.40756303e-01 3.15126050e-02 1.28151261e-01\n",
      "  4.20168067e-03 0.00000000e+00 5.46218487e-02]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create confusion matrix and classification report\n",
    "for_report = model.predict(test_x)\n",
    "out_pred = [np.argmax(x, axis=1) for x in for_report]\n",
    "out_pred = np.concatenate(out_pred, axis=0)\n",
    "\n",
    "y_ = [np.argmax(x, axis=1) for x in test_y]\n",
    "y_ = np.concatenate(y_, axis=0)\n",
    "\n",
    "cm = confusion_matrix(y_true=y_, y_pred=out_pred)\n",
    "print(cm)\n",
    "\n",
    "cr = classification_report(y_true=y_, y_pred=out_pred)\n",
    "print(cr)\n",
    "\n",
    "overall = classification_report(y_true=y_, y_pred=out_pred, output_dict=True)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=class_names, normalize=True, title='Normalized Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the average Precision, Recall and, F1-Score of different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comma, Period, Question Precision:\t0.761\n",
      "Comma, Period, Question Recall:\t\t0.696\n",
      "Comma, Period, Question F1-Score:\t0.727\n",
      "\n",
      "\n",
      "Overall Precision:\t0.650\n",
      "Overall Recall:\t\t0.435\n",
      "Overall F1-Score:\t0.468\n"
     ]
    }
   ],
   "source": [
    "# comma, period, question, exclaim, 3-dots\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for i in range(2, 7):\n",
    "    precision.append(overall[str(i)]['precision'])\n",
    "    recall.append(overall[str(i)]['recall'])\n",
    "    f1.append(overall[str(i)]['f1-score'])\n",
    "\n",
    "print('Comma, Period, Question Precision:\\t{:.3f}'.format(np.average(precision[:3])))\n",
    "print('Comma, Period, Question Recall:\\t\\t{:.3f}'.format(np.average(recall[:3])))\n",
    "print('Comma, Period, Question F1-Score:\\t{:.3f}'.format(np.average(f1[:3])))\n",
    "print('\\n')\n",
    "\n",
    "print('Overall Precision:\\t{:.3f}'.format(np.average(precision)))\n",
    "print('Overall Recall:\\t\\t{:.3f}'.format(np.average(recall)))\n",
    "print('Overall F1-Score:\\t{:.3f}'.format(np.average(f1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name='reference'> References </a>\n",
    "\n",
    "* [Bitbucket Repo]()\n",
    "* [How to build a PoS tagger](https://nlpforhackers.io/lstm-pos-tagger-keras/)\n",
    "* [Keras pre-trained word embeddings](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)\n",
    "* [Keras Documentation](https://keras.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
