{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "from random import randint\n",
    "from collections import Counter\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import random, torch, numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(w, h, f, p, s):\n",
    "    '''\n",
    "    Args:\n",
    "        w: Width of input\n",
    "        h: Height of input\n",
    "        f: Kernel width/height\n",
    "        p: Padding\n",
    "        s: Stride\n",
    "    '''\n",
    "    w = ((w-f+2*p)/s)+1\n",
    "    h = ((h-f+2*p)/s)+1\n",
    "    return ('Conv Shape:', [w, h])\n",
    "\n",
    "def get_seq_words(seq):\n",
    "    return seq.split()\n",
    "\n",
    "def get_max_len(doc):\n",
    "    max_len = 0\n",
    "    for x in doc:\n",
    "        if len(x) > max_len:\n",
    "            max_len = len(x)\n",
    "    return max_len\n",
    "\n",
    "def chunk_seq(seq):\n",
    "    chunked_seq = []\n",
    "    for i in range(0, len(seq), 5):\n",
    "        chunked_seq.append(seq[i:i+5])\n",
    "    return chunked_seq\n",
    "\n",
    "def get_label(seq):\n",
    "    labels = []\n",
    "    seq = seq.split()\n",
    "    if len(seq) < 5:\n",
    "        pass\n",
    "    else:\n",
    "        for word in seq:\n",
    "            if ',' in word:\n",
    "                labels.append(1)\n",
    "            elif '.' in word:\n",
    "                labels.append(2)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "        return labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28218860"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open('./data/processed/ted_data', 'r').read()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5030903"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = data.lower().split()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning.', 'how', 'are', \"you?(laughter)it's\"]\n"
     ]
    }
   ],
   "source": [
    "print(words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181375\n"
     ]
    }
   ],
   "source": [
    "vocab = tuple(set(words))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = get_max_len(words)\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006181\n"
     ]
    }
   ],
   "source": [
    "x = chunk_seq(words)\n",
    "sequences = [' '.join(seq) for seq in x]\n",
    "print(len(sequences))\n",
    "\n",
    "with open('aaa', 'w') as f:\n",
    "    for y in sequences:\n",
    "        f.write(y+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for seq in sequences:\n",
    "    seq = seq.split()\n",
    "    if len(seq) < 5:\n",
    "        pass\n",
    "    elif ',' in seq[2]:\n",
    "        labels.append([1,0,0])\n",
    "    elif '.' in seq[2]:\n",
    "        labels.append([0,1,0])\n",
    "    else:\n",
    "        labels.append([0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'[0, 0, 0]': 877643, '[0, 1, 0]': 57695, '[1, 0, 0]': 70842})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(str(lab) for lab in labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006180\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing data: 643956\n",
      "Number of validation data: 160988\n"
     ]
    }
   ],
   "source": [
    "train_data = sequences[:int(0.8*len(sequences))]\n",
    "train_labels = labels[:int(0.8*len(labels))]\n",
    "# test_data = sequences[-int(0.8*len(sequences)):]\n",
    "# test_labels = labels[-int(0.8*len(labels)):]\n",
    "\n",
    "partition = {}\n",
    "train_indices = []\n",
    "valid_indices = []\n",
    "\n",
    "print('Number of testing data: {}'.format(len(train_data)-valid_data))\n",
    "valid_data = int(0.2*len(train_data))\n",
    "print('Number of validation data: {}'.format(valid_data))\n",
    "\n",
    "for i in range((len(train_data)-valid_data)):\n",
    "    train_indices.append(i)\n",
    "    \n",
    "for i in range((len(train_data)-valid_data), len(train_data)):\n",
    "    valid_indices.append(i)\n",
    "    \n",
    "# Dictionary to hold indices of training and validation data\n",
    "partition['train'] = train_indices\n",
    "partition['valid'] = valid_indices\n",
    "\n",
    "train_valid_labels = {}\n",
    "for i in range(len(train_data)):\n",
    "    train_valid_labels[i] = labels[i]\n",
    "\n",
    "# num_train = len(train_data)\n",
    "# indices = list(range(num_train))\n",
    "# np.random.shuffle(indices)\n",
    "# split = int(np.floor(0.2 * num_train))\n",
    "# train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# # define samplers for obtaining training and validation batches\n",
    "# train_sampler = SubsetRandomSampler(train_idx)\n",
    "# valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# # prepare data loaders (combine dataset and sampler)\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=20,\n",
    "#     sampler=train_sampler, num_workers=0)\n",
    "# valid_loader = torch.utils.data.DataLoader(train_data, batch_size=20, \n",
    "#     sampler=valid_sampler, num_workers=0)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=20, \n",
    "#     num_workers=0)\n",
    "\n",
    "# print('Number of Train Data/Labels: {}/{}'.format(len(train_data), len(train_labels)))\n",
    "# print('Number of Validation Data/Labels: {}/{}'.format(len(valid_data), len(valid_labels)))\n",
    "# print('Number of Test Data/Labels: {}/{}'.format(len(test_data), len(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 1, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[1, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 1, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 1, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[1, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 1, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[1, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[1, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for x in range(100):\n",
    "    print(train_valid_labels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('awdad', 'w') as f:\n",
    "    for x in train_data:\n",
    "        f.write(x+'\\n')\n",
    "\n",
    "with open('czfez', 'w') as f:\n",
    "    for x in train_labels:\n",
    "        f.write(str(x)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embed): Embedding(181375, 64)\n",
      "  (dropout): Dropout(p=0.2)\n",
      "  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (full_1): Linear(in_features=1536, out_features=512, bias=True)\n",
      "  (output): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=181375, embedding_dim=64)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        # convolutional layer\n",
    "        # 5 x 64 x 1 = 320\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "        # 2 x 32 x 128 = 8192\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=0)\n",
    "        # 1 x 16 x 256 = 4096\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=0)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # 2048 > 256\n",
    "        self.full_1 = nn.Linear(512*1*3, 512)\n",
    "        self.output = nn.Linear(512, 10)                \n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(self.dropout(F.relu(self.conv1(x))))\n",
    "        print('Conv_1:', x.shape)\n",
    "        x = self.pool(self.dropout(F.relu(self.conv2(x))))\n",
    "        print('Conv_2:', x.shape)\n",
    "        x = self.pool(self.dropout(F.relu(self.conv3(x))))\n",
    "        print('Conv_3:', x.shape)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.dropout(F.relu(self.full_1(x)))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# create a complete CNN\n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 8 # you may increase this number to train a final model\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
