{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras, string, random, datetime, numpy as np, matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, InputLayer, Bidirectional, TimeDistributed, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_seq(seq, chunk_len):\n",
    "    chunked_seq = []\n",
    "    for i in range(0, len(seq), chunk_len):\n",
    "        chunked_seq.append(seq[i:i+chunk_len])\n",
    "    return chunked_seq\n",
    "\n",
    "def get_labels(seq):\n",
    "    labels_seq = []\n",
    "    seq = seq.split()\n",
    "    for i in range(len(seq)):\n",
    "        if ',' in seq[i]:\n",
    "            labels_seq.append('<comma>')\n",
    "        elif '.' in seq[i]:\n",
    "            labels_seq.append('<period>')\n",
    "        else:\n",
    "            labels_seq.append('<na>')\n",
    "    return labels_seq\n",
    "\n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set misc parameters\n",
    "current = datetime.datetime.now()\n",
    "date = current.strftime('%dd-%%mm')\n",
    "tb = TensorBoard(log_dir='./tf_logs/{}'.format(date), batch_size=64, write_graph=True, histogram_freq=0)\n",
    "\n",
    "# Look-up table to remove punctuations from data\n",
    "table = str.maketrans('', '', punctuation)\n",
    "\n",
    "# Set max sequence length\n",
    "max_seq_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: \t167697\n",
      "Number of labels: \t167697\n"
     ]
    }
   ],
   "source": [
    "# Load and process input/label data\n",
    "data = open('./data/processed/ted_data', 'r', encoding='utf-8').read()\n",
    "data = data.lower()\n",
    "data_split = data.split('\\n')\n",
    "all_data = ' '.join(data_split)\n",
    "words = all_data.split()\n",
    "\n",
    "# Chunk sequence\n",
    "x = chunk_seq(words, max_seq_len)\n",
    "sequences = [' '.join(seq) for seq in x]\n",
    "\n",
    "# Get sequence labels\n",
    "process_labels = [get_labels(seq) for seq in sequences]\n",
    "process_labels = [' '.join(seq) for seq in process_labels]\n",
    "\n",
    "# Remove punctuations\n",
    "sequences = [seq.translate(table) for seq in sequences]\n",
    "\n",
    "with open('./processed_input', 'w', encoding='utf-8') as f:\n",
    "    for x in sequences:\n",
    "        f.write(x+'\\n')\n",
    "\n",
    "with open('./processed_labels', 'w', encoding='utf-8') as f:\n",
    "    for x in process_labels:\n",
    "        f.write(x+'\\n')\n",
    "\n",
    "# Check number of sequences and labels\n",
    "print('Number of sequences: \\t{}'.format(len(sequences)))\n",
    "print('Number of labels: \\t{}'.format(len(process_labels)))\n",
    "\n",
    "y_labels = open('./processed_labels', 'r', encoding='utf-8').read()\n",
    "y_labels = y_labels.split('\\n')\n",
    "y_labels = y_labels[:-1]\n",
    "all_labels = ' '.join(y_labels)\n",
    "labels_tag = all_labels.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 108807\n"
     ]
    }
   ],
   "source": [
    "# Build words vocab\n",
    "all_data = ' '.join(sequences)\n",
    "words = all_data.split()\n",
    "words_in_vocab = Counter(words)\n",
    "vocab = sorted(words_in_vocab, key=words_in_vocab.get, reverse=True)\n",
    "\n",
    "# Skip most common word\n",
    "vocab_to_int = {word: index for index, word in enumerate(vocab, 2)}\n",
    "vocab_to_int['-PAD-'] = 0  # The special value used for padding\n",
    "vocab_to_int['-OOV-'] = 1  # The special value used for OOVs\n",
    "unique_vocab = len(vocab_to_int)\n",
    "print('Number of unique words:', unique_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: Counter({'<na>': 4387594, '<comma>': 354651, '<period>': 288658})\n",
      "Number of unique labels: 4\n",
      "{'<na>': 1, '<comma>': 2, '<period>': 3, '-PAD-': 0}\n"
     ]
    }
   ],
   "source": [
    "# Build labels vocab\n",
    "labels_in_vocab = Counter(labels_tag)\n",
    "labels_vocab = sorted(labels_in_vocab, key=labels_in_vocab.get, reverse=True)\n",
    "label_to_int = {t: i for i, t in enumerate(labels_vocab, 1)}\n",
    "label_to_int['-PAD-'] = 0  # The special value used to padding\n",
    "\n",
    "# Check labels\n",
    "no_classes = len(label_to_int)\n",
    "print('Class distribution:', Counter(labels_in_vocab))\n",
    "print('Number of unique labels:', no_classes)\n",
    "print(label_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sequence: day so it might be that this is not just a game it might be a way to decide our own fatethank youapplause\n",
      "Sample sequence: [   142     17     13    170     29      7     14     10     31     46\n",
      "      6    526     13    170     29      6     86      4   1222     42\n",
      "    160 108806   9653      0      0      0      0      0      0      0]\n",
      "Sample label: [3 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 3 3 0 0 0 0 0 0 0]\n",
      "Encoded label [[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Maximum sequence length: 30\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input sequences\n",
    "seq_int = []\n",
    "for seq in sequences:\n",
    "    seq_int.append([vocab_to_int[word] for word in seq.split()])\n",
    "\n",
    "# Pad input sequences\n",
    "pad_seq = pad_sequences(sequences=seq_int, maxlen=max_seq_len, padding='post', value=0)\n",
    "\n",
    "# Check sample sequence\n",
    "print('Sample sequence:', sequences[-1])\n",
    "print('Sample sequence:', pad_seq[-1])\n",
    "\n",
    "# Tokenize output labels\n",
    "lab_int = []\n",
    "for lab in y_labels:\n",
    "    lab_int.append([label_to_int[word] for word in lab.split()])\n",
    "\n",
    "# Pad input labels\n",
    "pad_labels = pad_sequences(sequences=lab_int, maxlen=max_seq_len, padding='post', value=0)\n",
    "encoded_labels = [to_categorical(i, num_classes=no_classes) for i in pad_labels]\n",
    "\n",
    "# Check sample label\n",
    "print('Sample label:', pad_labels[-1])\n",
    "print('Encoded label', encoded_labels[-1])\n",
    "\n",
    "# Check max seq length\n",
    "print(\"Maximum sequence length: {}\".format(max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all sequences and labels are at max sequence length \n",
    "assert len(pad_seq)==len(seq_int)\n",
    "assert len(pad_seq[0])==max_seq_len\n",
    "\n",
    "assert len(pad_labels)==len(lab_int)\n",
    "assert len(pad_labels[0])==max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Dataset: \t(134157, 30) 134157\n",
      "Testing Dataset: \t\t(33540, 30) 33540\n"
     ]
    }
   ],
   "source": [
    "# Split train and label dataset\n",
    "train_test_split_frac = 0.8\n",
    "split_index = int(0.8*len(pad_seq))\n",
    "\n",
    "# Split data into training, validation, and test data (features and labels, x and y)\n",
    "train_val_x, test_x = pad_seq[:split_index], pad_seq[split_index:]\n",
    "train_val_y, test_y = encoded_labels[:split_index], encoded_labels[split_index:]\n",
    "\n",
    "# print out the shapes of your resultant feature data\n",
    "print('Training/Validation Dataset: \\t{}'.format(train_val_x.shape), len(train_val_y))\n",
    "print('Testing Dataset: \\t\\t{}'.format(test_x.shape), len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 128)           13927296  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 30, 64)            24640     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 30, 128)           24704     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 30, 512)           788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 30, 4)             2052      \n",
      "=================================================================\n",
      "Total params: 14,767,172\n",
      "Trainable params: 14,767,172\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 93909 samples, validate on 40248 samples\n",
      "Epoch 1/2\n",
      "93909/93909 [==============================] - 147s 2ms/step - loss: 0.2847 - acc: 0.8976 - val_loss: 0.2459 - val_acc: 0.9058\n",
      "Epoch 2/2\n",
      "93909/93909 [==============================] - 146s 2ms/step - loss: 0.2115 - acc: 0.9201 - val_loss: 0.2312 - val_acc: 0.9109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ec72c6c3c8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model code\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=unique_vocab, output_dim=128, input_length=max_seq_len))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='SAME'))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, padding=\"SAME\"))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(no_classes, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])#, ignore_class_accuracy(0)])\n",
    "model.summary()\n",
    "model.fit(x=train_val_x, y=np.array(train_val_y), batch_size=64, epochs=2, validation_split=0.3, \n",
    "          shuffle=True, verbose=1, callbacks=[tb])\n",
    "\n",
    "# print('Saving Model')\n",
    "# model.save('model.h5')\n",
    "# print('Done')\n",
    "\n",
    "# scores = model.evaluate(x=test_x, y=np.array(test_y), verbose=1)\n",
    "# print('Accuracy: {}'.format(scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Index:\n",
      "[array([3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)]\n",
      "Prediction sequence:\n",
      "religion the words haram — meaning religiously prohibited — and aib — meaning culturally inappropriate — were exchanged carelessly as if they meant the same thing and had the same\n",
      "Prediction output:\n",
      "<period> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na> <na>\n"
     ]
    }
   ],
   "source": [
    "# Make prediction on a single sequence\n",
    "# Sequence to predict\n",
    "test_data = test_x[498]\n",
    "pred_x_seq = []\n",
    "for x in test_data:\n",
    "    for value, index in vocab_to_int.items():\n",
    "        if x == index:\n",
    "            pred_x_seq.append(value)\n",
    "\n",
    "# Predicted output\n",
    "pred_expand = model.predict(np.expand_dims(test_data, axis=0))\n",
    "pred_y = []\n",
    "for y in pred_expand:\n",
    "    pred_y.append(np.argmax(y, axis=1))\n",
    "print('Predictions Index:')\n",
    "print(pred_y)\n",
    "\n",
    "pred_y_seq = []\n",
    "for x in pred_y:\n",
    "    for y in x:\n",
    "        for value, index in label_to_int.items():\n",
    "            if y == index:\n",
    "                pred_y_seq.append(value)\n",
    "\n",
    "print('Prediction sequence:')            \n",
    "print(' '.join(pred_x_seq))\n",
    "print('Prediction output:')\n",
    "print(' '.join(pred_y_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33540, 30)\n",
      "(33540, 30)\n"
     ]
    }
   ],
   "source": [
    "# # WIP for CM and CR\n",
    "# for_report = model.predict(test_x)\n",
    "# fr_ = np.array([np.argmax(x, axis=1) for x in for_report])\n",
    "# print(fr_.shape)\n",
    "\n",
    "# y_ = np.array([np.argmax(x, axis=1) for x in test_y])\n",
    "# print(y_.shape)\n",
    "\n",
    "# cm = tf.confusion_matrix(labels=y_[50], predictions=fr_[50], num_classes=4)\n",
    "# # print('Classification Report:')\n",
    "# # # print(cr)\n",
    "# # print('Confusion Matrix:')\n",
    "# # print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_report = model.predict(test_x)\n",
    "fr_ = [np.argmax(x, axis=1) for x in for_report]\n",
    "fr_ = np.concatenate(fr_, axis=0)\n",
    "\n",
    "y_ = [np.argmax(x, axis=1) for x in test_y]\n",
    "y_ = np.concatenate(y_, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,      7,      0,      0],\n",
       "       [     0, 860577,  10728,  10566],\n",
       "       [     0,  28932,  23236,  15730],\n",
       "       [     0,  16558,   5660,  34206]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gggg = confusion_matrix(y_, fr_)\n",
    "gggg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.00      0.00      0.00         7\\n           1       0.95      0.98      0.96    881871\\n           2       0.59      0.34      0.43     67898\\n           3       0.57      0.61      0.59     56424\\n\\n   micro avg       0.91      0.91      0.91   1006200\\n   macro avg       0.53      0.48      0.49   1006200\\nweighted avg       0.90      0.91      0.91   1006200\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hhh = classification_report(y_, fr_)\n",
    "hhh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
