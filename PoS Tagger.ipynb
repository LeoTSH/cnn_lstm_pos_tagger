{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras, random, numpy as np, matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, InputLayer, Bidirectional, TimeDistributed, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_seq(seq, chunk_len):\n",
    "    chunked_seq = []\n",
    "    for i in range(0, len(seq), chunk_len):\n",
    "        chunked_seq.append(seq[i:i+chunk_len])\n",
    "    return chunked_seq\n",
    "\n",
    "def get_labels(seq):\n",
    "    labels_seq = []\n",
    "    seq = seq.split()\n",
    "    for i in range(len(seq)):\n",
    "        if ',' in seq[i]:\n",
    "            labels_seq.append('<comma>')\n",
    "        elif '.' in seq[i]:\n",
    "            labels_seq.append('<period>')\n",
    "        else:\n",
    "            labels_seq.append('<na>')\n",
    "    return labels_seq\n",
    "\n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy\n",
    "\n",
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)]) \n",
    "        token_sequences.append(token_sequence) \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28218860"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb = TensorBoard(log_dir='./tf_logs/', batch_size=64, write_graph=True, histogram_freq=0)\n",
    "data = open('./data/processed/ted_data', 'r', encoding='utf-8').read()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.lower()\n",
    "data_split = data.split('\\n')\n",
    "all_data = ' '.join(data_split)\n",
    "words = all_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk sequence\n",
    "x = chunk_seq(words, 15)\n",
    "sequences = [' '.join(seq) for seq in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sequence labels\n",
    "process_labels = [get_labels(seq) for seq in sequences]\n",
    "process_labels = [' '.join(seq) for seq in process_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: \t335394\n",
      "Number of labels: \t335394\n"
     ]
    }
   ],
   "source": [
    "# Check number of sequences and labels\n",
    "print('Number of sequences: \\t{}'.format(len(sequences)))\n",
    "print('Number of labels: \\t{}'.format(len(process_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = open('./pos_labels', 'r').read()\n",
    "y_labels = y_labels.split('\\n')\n",
    "all_labels = ' '.join(y_labels)\n",
    "labels_tag = all_labels.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 181377\n"
     ]
    }
   ],
   "source": [
    "# Build words vocab\n",
    "words_in_vocab = Counter(words)\n",
    "vocab = sorted(words_in_vocab, key=words_in_vocab.get, reverse=True)\n",
    "\n",
    "# Skip most common word\n",
    "vocab_to_int = {word: index for index, word in enumerate(vocab, 2)}\n",
    "vocab_to_int['-PAD-'] = 0  # The special value used for padding\n",
    "vocab_to_int['-OOV-'] = 1  # The special value used for OOVs\n",
    "unique_vocab = len(vocab_to_int)\n",
    "print('Number of unique words:', len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 4\n",
      "{'<na>': 1, '<comma>': 2, '<period>': 3, '-PAD-': 0}\n"
     ]
    }
   ],
   "source": [
    "# Build labels vocab\n",
    "labels_in_vocab = Counter(labels_tag)\n",
    "labels_vocab = sorted(labels_in_vocab, key=labels_in_vocab.get, reverse=True)\n",
    "label_to_int = {t: i for i, t in enumerate(labels_vocab, 1)}\n",
    "label_to_int['-PAD-'] = 0  # The special value used to padding\n",
    "\n",
    "# Check labels\n",
    "no_classes = len(label_to_int)\n",
    "print('Number of unique labels:', no_classes)\n",
    "print(label_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sequence: in education, you're not asked.(laughter)and you're never asked back, curiously. that's strange to me. but\n",
      "Sample sequence: [8, 1500, 110, 32, 75874, 110, 164, 311, 859, 75875, 61, 1381, 4, 309, 21]\n",
      "Sample label: [1, 2, 1, 1, 3, 1, 1, 1, 2, 3, 1, 1, 1, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input sequences\n",
    "seq_int = []\n",
    "for seq in sequences:\n",
    "    seq_int.append([vocab_to_int[word] for word in seq.split()])\n",
    "print('Sample sequence:', sequences[10])\n",
    "print('Sample sequence:', seq_int[10])\n",
    "\n",
    "# Tokenize output labels\n",
    "lab_int = []\n",
    "for lab in y_labels:\n",
    "    lab_int.append([label_to_int[word] for word in lab.split()])\n",
    "print('Sample label:', lab_int[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 15\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Check max seq length\n",
    "seq_len = Counter([len(seq) for seq in seq_int])\n",
    "max_seq_len = max(seq_len)\n",
    "print(\"Maximum sequence length: {}\".format(max(seq_len)))\n",
    "\n",
    "encoded_labels = [to_categorical(i, num_classes=no_classes) for i in lab_int]\n",
    "print(encoded_labels[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to 15 or sequence length, post padding\n",
    "features = np.zeros((len(seq_int), 15), dtype=int)\n",
    "\n",
    "for i, row in enumerate(seq_int):\n",
    "    features[i, :len(row)] = np.array(row)[:15]\n",
    "\n",
    "# Check that all sequences at at length 5\n",
    "assert len(features)==len(seq_int)\n",
    "assert len(features[0])==15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test amount: \t33539\n",
      "Training Dataset: \t(268315, 15)\n",
      "Validation Dataset: \t(33539, 15)\n",
      "Testing Dataset: \t(33540, 15)\n"
     ]
    }
   ],
   "source": [
    "train_test_split_frac = 0.8\n",
    "split_index = int(0.8*len(features))\n",
    "\n",
    "# Split data into training, validation, and test data (features and labels, x and y)\n",
    "train_x, left_over_x = features[:split_index], features[split_index:]\n",
    "train_y, left_over_y = encoded_labels[:split_index], encoded_labels[split_index:]\n",
    "\n",
    "val_test_index = int(0.5*len(left_over_x))\n",
    "print('Validation/Test amount: \\t{}'.format(val_test_index))\n",
    "\n",
    "val_x, test_x = left_over_x[:val_test_index], left_over_x[val_test_index:]\n",
    "val_y, test_y = left_over_y[:val_test_index], left_over_y[val_test_index:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print('Training Dataset: \\t{}'.format(train_x.shape))\n",
    "print('Validation Dataset: \\t{}'.format(val_x.shape))\n",
    "print('Testing Dataset: \\t{}'.format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 15, 128)           23216256  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 15, 512)           788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 15, 4)             2052      \n",
      "=================================================================\n",
      "Total params: 24,006,788\n",
      "Trainable params: 24,006,788\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 268315 samples, validate on 33539 samples\n",
      "Epoch 1/2\n",
      "268315/268315 [==============================] - 245s 915us/step - loss: 0.0544 - acc: 0.9785 - ignore_accuracy: 0.9786 - val_loss: 0.0287 - val_acc: 0.9870 - val_ignore_accuracy: 0.9870\n",
      "Epoch 2/2\n",
      "268315/268315 [==============================] - 245s 912us/step - loss: 0.0013 - acc: 0.9995 - ignore_accuracy: 0.9995 - val_loss: 0.0338 - val_acc: 0.9855 - val_ignore_accuracy: 0.9855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19c90197588>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# model.add(InputLayer(input_shape=(max_seq_len,)))\n",
    "model.add(Embedding(input_dim=unique_vocab, output_dim=128, input_length=max_seq_len))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(no_classes, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy', ignore_class_accuracy(0)])\n",
    "model.summary()\n",
    "model.fit(x=train_x, y=np.array(train_y), batch_size=64, epochs=2, validation_data=(val_x, np.array(val_y)),\n",
    "          shuffle=True, verbose=1, callbacks=[tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scores = model.evaluate(x=test_x, y=np.array(test_y), verbose=1)\n",
    "# print('Accuracy: {}'.format(scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  78   12   27    3  102 1578   64   28  120  328 1418   30   42  756\n",
      "    4]\n",
      "[[1.24522703e-08 9.99999762e-01 2.59608925e-07 1.13188803e-09]\n",
      " [1.12887697e-07 9.99997735e-01 1.11772795e-06 1.02823708e-06]\n",
      " [4.48813609e-08 9.99997735e-01 1.83559882e-06 3.79251588e-07]\n",
      " [4.36679226e-09 9.99999881e-01 7.78676181e-08 1.47648080e-08]\n",
      " [1.14226841e-08 9.99999881e-01 3.88848811e-08 2.28565380e-08]\n",
      " [3.14043431e-12 1.10060772e-09 1.00000000e+00 4.24195470e-08]\n",
      " [5.87399862e-09 1.00000000e+00 4.38118342e-09 3.88034188e-10]\n",
      " [1.01635784e-08 1.00000000e+00 2.57367585e-08 2.81074697e-09]\n",
      " [1.13985914e-08 9.99999881e-01 1.25559040e-07 1.12288525e-08]\n",
      " [1.68999978e-08 9.99999881e-01 6.33630606e-08 1.99309227e-08]\n",
      " [6.29304031e-09 1.00000000e+00 1.92707201e-08 4.29327374e-09]\n",
      " [1.43760097e-08 9.99999881e-01 1.18626296e-07 2.66695039e-08]\n",
      " [4.13458494e-08 9.99999523e-01 3.28858079e-07 9.41037612e-08]\n",
      " [1.65243552e-08 9.99999881e-01 5.85385749e-08 1.31199496e-08]\n",
      " [4.84594338e-08 1.00000000e+00 3.38830866e-08 4.70052131e-09]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_x)\n",
    "print(test_x[0])\n",
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cents on the dollar comes from. but in the federal government, where salaries are pinned\n",
      "<na> <na> <na> <na> <na> <period> <na> <na> <na> <na> <comma> <na> <na> <na> <na>\n"
     ]
    }
   ],
   "source": [
    "# Sequence to predict\n",
    "pred_x_seq = []\n",
    "for x in test_x[400]:\n",
    "    for i, v in vocab_to_int.items():\n",
    "        if v == x:\n",
    "            pred_x_seq.append(i)\n",
    "print(' '.join(pred_x_seq))\n",
    "\n",
    "# Predicted output\n",
    "pred_y = pred[400].argmax(axis=1)\n",
    "pred_y_seq = []\n",
    "for x in pred_y:\n",
    "    for i, v in label_to_int.items():\n",
    "        if v == x:\n",
    "            pred_y_seq.append(i)\n",
    "print(' '.join(pred_y_seq))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
